<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<title>Excavating AI: Precedent Study</title>
<style>
body {
  font-family: Arial, sans-serif;
  background: #fff;
  color: #111;
  margin: 0;
  padding: 0;
}
header {
  background: #222;
  color: #fff;
  padding: 1em;
  text-align: center;
}
nav {
  display: flex;
  justify-content: center;
  background: #eee;
}
nav button {
  background: none;
  border: none;
  padding: 1em;
  cursor: pointer;
  font-size: 1em;
}
nav button:hover {
  background: #ddd;
}
nav button.active {
  background: #ccc;
}
section {
  display: none;
  padding: 2em;
}
section.active {
  display: block;
}
h2 {
  margin-top: 1em;
  font-size: 1.2em;
  background: #f0f0f0;
  padding: 0.4em;
  border-radius: 4px;
}
p, li {
  font-size: 1em;
  margin: 0.2em 0;
}
ul {
  list-style: disc;
  padding-left: 1.2em;
}
table {
  border-collapse: collapse;
  width: 100%;
  margin-top: 0.5em;
  background: #f9f9f9;
}
th, td {
  border: 1px solid #ccc;
  padding: 0.4em;
  text-align: left;
}
a {
  color: #0645ad;
}
#timeline {
  width: 100%;
  max-width: 900px;
  margin: 1em auto;
}
#timeline svg {
  border-radius: 6px;
  box-shadow: 0 0 5px #ccc;
  background: #f9f9f9;
}

/* Ontological Analysis styles */
#ontological-analysis {
  font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif;
  max-width: 900px;
  margin: 2rem auto;
  padding: 0 1rem;
  color: #222;
}
#ontological-analysis h2 {
  text-align: center;
  font-weight: 700;
  margin-bottom: 1.5rem;
  font-size: 2rem;
}
#ontological-analysis ul {
  list-style-type: none;
  padding-left: 0;
}
#ontological-analysis li {
  margin-bottom: 2.5rem;
  line-height: 1.6;
}
#ontological-analysis strong {
  font-size: 1.2rem;
  color: #004080;
}
.visual-placeholder {
  margin-top: 1rem;
  border: 2px dashed #ccc;
  background-color: #f8f8f8;
  min-height: 180px;
  display: flex;
  justify-content: center;
  align-items: center;
  color: #aaa;
  font-style: italic;
}

/* Timeline text styles */
.timeline-group text {
  font-size: 0.9em;
  fill: #111;
}
.timeline-group .year {
  font-weight: bold;
}
.timeline-event {
  fill: #0645ad;
}
</style>
<script src="https://d3js.org/d3.v7.min.js"></script>
</head>
<body>

<header>
  <h1>Excavating AI: Precedent Study</h1>
</header>

<nav>
  <button id="tab1" class="active" onclick="showSection('dataCollection')">Data Collection</button>
  <button id="tab2" onclick="showSection('layersAnalysis')">Layers of Analysis</button>
  <button id="tab3" onclick="showSection('diagramAnalysis')">Diagram & Further Analysis</button>
</nav>

<!-- Data Collection -->
<section id="dataCollection" class="active">
  <h2>Authors</h2>
  <ul>
    <li>Kate Crawford – Researcher at Microsoft Research, co-founder of the AI Now Institute</li>
    <li>Trevor Paglen – Artist and geographer known for investigating mass surveillance and data politics</li>
  </ul>

  <h2>Year</h2>
  <p>2019</p>

  <h2>Format</h2>
  <ul>
    <li>Online interactive essay</li>
    <li>Exhibition (Fondazione Prada, Milan; Barbican Centre, London)</li>
    <li>Research + visual critique</li>
  </ul>

  <h2>Audience</h2>
  <ul>
    <li>Designers, Artists</li>
    <li>Technologists</li>
    <li>Researchers in AI, media, ethics</li>
    <li>General public interested in algorithmic justice, facial recognition, or surveillance</li>
  </ul>

  <h2>Project Description</h2>
  <p>
    Excavating AI reveals how large-scale image datasets used to train machine learning systems are not neutral. It critiques the taxonomies, labels, and biases that shape computer vision through datasets like ImageNet, MegaFace, and Microsoft's facial recognition dataset. By visualizing how people are sorted and classified, the project challenges the ethical foundations of AI technologies.
  </p>

  <h2>Key Data</h2>
  <table>
  <thead>
  <tr>
    <th>Dataset</th>
    <th>Images</th>
    <th>Purpose</th>
  </tr>
  </thead>
  <tbody>
  <tr>
    <td>ImageNet</td>
    <td>~14 million</td>
    <td>Object/image classification</td>
  </tr>
  <tr>
    <td>MegaFace</td>
    <td>~4 million</td>
    <td>Facial recognition training</td>
  </tr>
  <tr>
    <td>MS-Celeb-1M</td>
    <td>10 million+</td>
    <td>Face identification & tagging</td>
  </tr>
  <tr>
    <td colspan="3">Also referenced: WordNet, UTKFace, web-scraped photo datasets, and classification taxonomies.</td>
  </tr>
  </tbody>
  </table>

  <h2>Bibliography: Writings by the Authors</h2>
  <ul>
    <li>Crawford, Kate, and Paglen, Trevor. "Excavating AI: The Politics of Images in Machine Learning Training Sets." AI Now Institute, 2019. <a href="https://excavating.ai" target="_blank">https://excavating.ai</a></li>
    <li>Crawford, Kate. Atlas of AI. Yale University Press, 2021.</li>
    <li>Paglen, Trevor. From Apple to Anomaly. Barbican Centre, London, 2019. <a href="https://www.barbican.org.uk/s/trevorpaglen/" target="_blank">https://www.barbican.org.uk/s/trevorpaglen/</a></li>
  </ul>

  <h2>Bibliography: Writings by Others</h2>
  <ul>
    <li>Broussard, Meredith. Artificial Unintelligence. MIT Press, 2018.</li>
    <li>AI Now Institute. AI Now Report 2018. <a href="https://ainowinstitute.org/AI_Now_2018_Report.pdf" target="_blank">https://ainowinstitute.org/AI_Now_2018_Report.pdf</a></li>
    <li>Buolamwini, Joy, & Gebru, Timnit. "Gender Shades." 2018. <a href="http://proceedings.mlr.press/v81/buolamwini18a.html" target="_blank">http://proceedings.mlr.press/v81/buolamwini18a.html</a></li>
    <li>Solon, Olivia. Facial recognition's 'dirty little secret'. NBC News, 2019. <a href="https://www.nbcnews.com/tech/internet/facial-recognition-s-dirty-little-secret-millions-online-photos-scraped-n981921" target="_blank">https://www.nbcnews.com/tech/internet/facial-recognition-s-dirty-little-secret-millions-online-photos-scraped-n981921</a></li>
    <li>Locker, Melissa. Microsoft, Duke, and Stanford quietly delete databases. Fast Company, 2019. <a href="https://www.fastcompany.com/90360490/microsoft-duke-and-stanford-quietly-delete-databases-with-millions-of-faces" target="_blank">https://www.fastcompany.com/90360490/microsoft-duke-and-stanford-quietly-delete-databases-with-millions-of-faces</a></li>
    <li>Satisky, Jake. A Duke study recorded thousands of students' faces. Duke Chronicle, 2019. <a href="https://www.dukechronicle.com/article/2019/06/duke-university-facial-recognition-data-set-study-surveillance-video-students-china-uyghur" target="_blank">https://www.dukechronicle.com/article/2019/06/duke-university-facial-recognition-data-set-study-surveillance-video-students-china-uyghur</a></li>
    <li>Murgia, Madhumita. Who's using your face? Financial Times, 2019. <a href="https://www.ft.com/content/cf19b956-60a2-11e9-b285-3acd5d43599e" target="_blank">https://www.ft.com/content/cf19b956-60a2-11e9-b285-3acd5d43599e</a></li>
    <li>Mitchell, W. J. T. Picture Theory. University of Chicago Press, 2007.</li>
    <li>Prabhu, Vinay Uday, & Birhane, Abeba. Large Image Datasets. arXiv, 2021. <a href="https://arxiv.org/abs/2106.11342" target="_blank">https://arxiv.org/abs/2106.11342</a></li>
    <li>Birhane, Abeba, & Prabhu, Vinay Uday. Multimodal datasets. arXiv, 2021. <a href="https://arxiv.org/abs/2103.01950" target="_blank">https://arxiv.org/abs/2103.01950</a></li>
  </ul>
</section>

<!-- Layers of Analysis -->
<section id="layersAnalysis">
<h2>Ontological Analysis</h2>
  <p>
    Excavating AI is a meticulous investigation into how machine learning systems inherit and perpetuate cultural assumptions through the images used to train them. Rather than treating datasets as neutral resources, the project unpacks their hidden architectures, taxonomies, and the ways they encode power, identity, and bias. Each section visually and critically exposes a layer of the infrastructures that sustain computer vision. The project employs a forensic approach to dataset analysis, systematically examining the cultural, political, and economic dimensions of AI training data to reveal how seemingly objective technological systems are actually embedded with historical prejudices and contemporary power dynamics.
  </p>

  <ul>
  <li>
    <strong>Training AI</strong><br/>
    The project begins by comparing artworks to show how images can hold many meanings. Agnes Martin's <em>White Flower</em> and Magritte's <em>The Treachery of Images</em> reveal how labels never fully capture what an image is or does. This points out that AI systems rely on naming and sorting to make sense of pictures. But the gap between an image and its label is always there. This gap is a core problem in supervised learning. It helps explain why AI can misread or oversimplify visual experience. The comparison demonstrates the fundamental epistemological challenge at the heart of computer vision: the impossibility of perfectly translating visual experience into discrete categories. Martin's minimalist abstraction and Magritte's conceptual play with representation both highlight how meaning in images is contextual, subjective, and resistant to fixed classification—precisely the qualities that AI systems struggle to process. This opening establishes the theoretical foundation for understanding why bias in AI is not simply a technical glitch but a structural feature of how machine learning systems attempt to make sense of visual complexity.
    <div style="display: flex; justify-content: center; gap: 1em; margin-top: 1em;">
      <img src="trainingai2.jpg" alt="White Flower by Agnes Martin, 1960" style="width: 35%; border-radius: 4px;" />
      <img src="trainingai1.jpg" alt="The Treachery of Images by René Magritte, 1929" style="width: 35%; border-radius: 4px;" />
    </div>
  </li>

  <li>
    <strong>Anatomy of a Training Set</strong><br/>
    Using ImageNet as an example, the project shows how millions of pictures are sorted and labeled by paid workers on Mechanical Turk. These platforms are part of the hidden labor behind datasets. One image shows the interface where workers assign tags. Another highlights the label "Hermaphrodite," which exposes harmful and outdated ideas still embedded in the system. This reveals that bias is not rare but common. Data collection always carries traces of culture and prejudice. The section exposes the invisible infrastructure of AI training, revealing how the massive scale of dataset creation relies on precarious, underpaid labor. The Mechanical Turk interface becomes a window into the human cost of AI development, showing how workers are paid pennies to make decisions that will shape how machines understand the world. The "Hermaphrodite" label exemplifies how historical prejudices become embedded in supposedly neutral technological systems, demonstrating that bias is not an accident but a feature of how cultural assumptions are encoded into data.
    <div style="display: flex; justify-content: center; gap: 1em; margin-top: 1em;">
      <img src="anatomy2.png" alt="Amazon Turk labeling interface for ImageNet" style="width: 35%; border-radius: 4px;" />
      <img src="anatomy1.jpg" alt="ImageNet category 'Hermaphrodite' within Person taxonomy" style="width: 35%; border-radius: 4px;" />
    </div>
  </li>

  <li>
    <strong>Categories and Image Selections from "Person" Classes, ImageNet</strong><br/>
    The project gathers examples of ImageNet's "Person" categories like "Accused," "Kleptomaniac," and "Anti-Semite." The faces are partly covered to show that these labels turn real people into harmful types. This evidence shows how classification systems create stereotypes. The images prove that bias is structural, not just an accident. They show how datasets can reproduce stigma and exclusion. This problem is central to computer vision. This section provides the most damning evidence of how AI systems perpetuate historical discrimination. By collecting examples of harmful classifications and showing the real people behind these labels, the project demonstrates how machine learning systems can transform individuals into harmful stereotypes. The redacted faces serve as both protection for the subjects and a powerful visual metaphor for how AI systems reduce complex human identities to problematic categories. This evidence shows that bias in AI is not a bug but a feature of how classification systems work—they inevitably reflect and amplify the prejudices of their creators and the societies they emerge from.
    <div style="display: flex; justify-content: center; gap: 1em; margin-top: 1em;">
      <img src="cat2.JPEG" alt="Selections from ImageNet 'Person' category" style="width: 35%; border-radius: 4px;" />
      <img src="cat1.jpg" alt="Labeled Images from ImageNet with redacted faces" style="width: 35%; border-radius: 4px;" />
    </div>
  </li>

  <li>
    <strong>UTK: Making Race and Gender from Faces</strong><br/>
    The UTKFace dataset shows how AI systems assign simple labels like "White Male" or "Indian Male" to people's faces. This practice flattens complex identities into fixed categories. The project explains how these labels echo the history of racial pseudoscience. The images make this process visible. They show how old ideas about difference get encoded into new technologies. This raises questions about who decides how identity is defined. This section connects contemporary AI practices to the long history of scientific racism and eugenics. The UTKFace dataset's classification scheme—with its binary gender categories and racial classifications—demonstrates how 19th-century pseudoscientific ideas about human difference persist in 21st-century technology. The project shows how these classifications are not neutral scientific categories but political decisions that reflect specific historical and cultural contexts. By making the classification process visible, it reveals how AI systems can perpetuate harmful ideas about human difference while appearing to be objective and scientific.
    <div style="display: flex; justify-content: center; gap: 1em; margin-top: 1em;">
      <img src="utk1.jpg" alt="Sample images from UTKFace dataset" style="width: 35%; border-radius: 4px;" />
      <img src="utk2.jpg" alt="Classification scheme used in UTKFace dataset" style="width: 35%; border-radius: 4px;" />
    </div>
  </li>

  <li>
    <strong>IBM's Diversity in Faces</strong><br/>
    IBM tried to build a fairer dataset by adding details about facial symmetry, craniofacial landmarks, and skin tone. But the project shows how even these efforts still divide people into measurable traits. The dataset still uses binary gender and fixed labels. This example proves that even improvements can keep old ideas in place. The images show how fairness can become another form of control. This challenges simple claims about bias-free AI. This section examines the limitations of technical solutions to social problems. IBM's "Diversity in Faces" dataset represents an attempt to address bias through better measurement and more diverse sampling. However, the project reveals how even well-intentioned efforts can perpetuate problematic assumptions about human difference. The focus on measurable traits like facial symmetry and skin tone demonstrates how attempts to be "fair" can still rely on reductive and potentially harmful ways of categorizing people. This example shows that the problem is not just about better data collection but about fundamentally rethinking how we approach human difference in AI systems.
    <div style="display: flex; justify-content: center; margin-top: 1em;">
      <img src="ibm.png" alt="IBM Diversity in Faces dataset example" style="width: 60%; border-radius: 4px;" />
    </div>
  </li>

  <li>
    <strong>Epistemics of Training Sets</strong><br/>
    This section questions the idea that visual data can be perfectly sorted or fully understood. Excavating AI argues that datasets always carry the worldviews and politics of their makers. Every choice about labels, categories, and images reflects power. The project shows that knowledge itself is shaped by culture and context. This raises deeper questions about objectivity. It invites us to think about what data leaves out. This theoretical section provides the philosophical foundation for understanding why AI bias is inevitable. It challenges the assumption that data can be neutral or objective, arguing instead that all knowledge production is shaped by cultural, political, and economic factors. The section draws on critical theory to show how the very act of classification is a form of power—deciding what counts as knowledge and how it should be organized. This perspective helps explain why technical solutions to AI bias often fail: they don't address the fundamental ways in which knowledge itself is constructed through power relations.
  </li>

  <li>
    <strong>Missing Persons</strong><br/>
    A photo from the MS-Celeb dataset shows the millions of faces that were removed after backlash over consent and privacy. This removal is a reminder that AI systems are built on unsteady ground. The disappearance of data is itself a record of conflict. It shows how power and ethics collide in dataset construction. The project uses this absence as evidence. It calls for accountability and transparency. This section examines the aftermath of dataset controversies and what they reveal about the ethics of AI development. The removal of the MS-Celeb dataset after public outcry demonstrates how AI systems are built on contested ground—data that was collected without proper consent and used without permission. The "missing" faces become a powerful metaphor for the people who are excluded, exploited, or erased in the process of building AI systems. This example shows how questions of consent, privacy, and human rights are central to AI development, not peripheral concerns that can be addressed later.
    <div style="display: flex; justify-content: center; margin-top: 1em;">
      <img src="missp.jpg" alt="Example image from MS CELEB dataset" style="width: 60%; border-radius: 4px;" />
    </div>
  </li>

  <li>
    <strong>Conclusion: Who Decides?</strong><br/>
    The project ends with a photo of Memphis sanitation workers holding "I AM A MAN" signs. This moment connects the right to name yourself to the ethics of AI. Excavating AI argues that labeling is always about power. Just as people have fought for the right to define their identity, AI systems define people without their say. The work asks who gets to decide what names, categories, and labels mean. It shows how this question links past and present struggles. The conclusion connects the technical analysis of AI bias to broader historical struggles for human dignity and self-determination. The Memphis sanitation workers' protest becomes a powerful metaphor for the right to define one's own identity—a right that AI systems systematically violate by imposing classifications without consent. This ending situates the project within a long history of resistance to dehumanizing systems of classification, from scientific racism to mass surveillance. It suggests that addressing AI bias requires not just technical fixes but fundamental changes in how we think about human dignity, autonomy, and the right to self-definition in the digital age.
    <div style="display: flex; justify-content: center; margin-top: 1em;">
      <img src="conc.jpg" alt="Photograph from Memphis Sanitation Workers Strike, 1968" style="width: 60%; border-radius: 4px;" />
    </div>
  </li>
  </ul>

  <h2>Historical and Contextual Analysis</h2>
  <p style="margin-top: 1em; max-width: 900px; margin-left: auto; margin-right: auto;">
  This timeline situates <em>Excavating AI</em> (2019) within the critical discourse on AI ethics, dataset transparency, and facial recognition bias from 2018 through 2025. It highlights key studies, publications, and events that have shaped ongoing debates. <strong>Hover over each point in the timeline to read detailed insights into these pivotal moments.</strong>
  </p>
  <div id="timeline"></div>

<h2>Visual and Aesthetic Representation</h2>

<table style="width: 100%; border-collapse: collapse; margin: 1.5rem 0; background: #f8f9fa; border-radius: 8px; overflow: hidden; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
  <thead>
    <tr style="background: linear-gradient(90deg, #2c3e50, #34495e); color: white;">
      <th style="padding: 0.8rem; text-align: left; font-weight: 600; width: 20%;">Visual Element</th>
      <th style="padding: 0.8rem; text-align: left; font-weight: 600; width: 35%;">Purpose & Function</th>
      <th style="padding: 0.8rem; text-align: left; font-weight: 600; width: 25%;">Critical Effect</th>
      <th style="padding: 0.8rem; text-align: left; font-weight: 600; width: 20%;">Artistic Influences</th>
    </tr>
  </thead>
  <tbody>
          <tr style="border-bottom: 1px solid #dee2e6;">
        <td style="padding: 0.8rem; font-weight: 600; color: #2c3e50;">Minimalist Typography</td>
        <td style="padding: 0.8rem;">Clean, readable fonts that prioritize content over decoration, ensuring accessibility and focus on dataset content</td>
        <td style="padding: 0.8rem; color: #000;">Emphasizes raw data over aesthetic embellishment, foregrounds critical analysis</td>
        <td style="padding: 0.8rem;">Swiss Design, Bauhaus principles, Information Design</td>
      </tr>
      <tr style="border-bottom: 1px solid #dee2e6; background: #ffffff;">
        <td style="padding: 0.8rem; font-weight: 600; color: #2c3e50;">Subdued Grayscale Palette</td>
        <td style="padding: 0.8rem;">Muted colors that emphasize documentary nature of content, creating clinical, systematic presentation</td>
        <td style="padding: 0.8rem; color: #000;">Reinforces forensic aesthetic, mimics investigative documentation</td>
        <td style="padding: 0.8rem;">Documentary photography, Conceptual art, Institutional critique</td>
      </tr>
      <tr style="border-bottom: 1px solid #dee2e6;">
        <td style="padding: 0.8rem; font-weight: 600; color: #2c3e50;">Documentary Photography</td>
        <td style="padding: 0.8rem;">Raw, unedited images from datasets showing real people and objects as they appear in training data</td>
        <td style="padding: 0.8rem; color: #000;">Exposes human cost and lack of consent in data collection</td>
        <td style="padding: 0.8rem;">Martha Rosler, Allan Sekula, Susan Meiselas</td>
      </tr>
      <tr style="border-bottom: 1px solid #dee2e6; background: #ffffff;">
        <td style="padding: 0.8rem; font-weight: 600; color: #2c3e50;">Taxonomic Grids</td>
        <td style="padding: 0.8rem;">Dense arrangements of images organized by classification categories, showing systematic categorization</td>
        <td style="padding: 0.8rem; color: #000;">Reveals impersonal nature of AI classification systems</td>
        <td style="padding: 0.8rem;">Hans Haacke, Bernd & Hilla Becher, Conceptual art</td>
      </tr>
      <tr style="border-bottom: 1px solid #dee2e6;">
        <td style="padding: 0.8rem; font-weight: 600; color: #2c3e50;">Redacted Faces</td>
        <td style="padding: 0.8rem;">Partially obscured human faces to protect privacy while showing subjects of harmful labels</td>
        <td style="padding: 0.8rem; color: #000;">Creates powerful metaphor for how AI reduces people to categories</td>
        <td style="padding: 0.8rem;">Surveillance art, Privacy activism, Forensic aesthetics</td>
      </tr>
      <tr style="border-bottom: 1px solid #dee2e6; background: #ffffff;">
        <td style="padding: 0.8rem; font-weight: 600; color: #2c3e50;">Interface Screenshots</td>
        <td style="padding: 0.8rem;">Images of Mechanical Turk labeling interfaces and dataset management tools</td>
        <td style="padding: 0.8rem; color: #000;">Exposes hidden labor and human decisions behind AI systems</td>
        <td style="padding: 0.8rem;">Harun Farocki, Trevor Paglen, Surveillance studies</td>
      </tr>
      <tr style="border-bottom: 1px solid #dee2e6;">
        <td style="padding: 0.8rem; font-weight: 600; color: #2c3e50;">Comparative Layouts</td>
        <td style="padding: 0.8rem;">Side-by-side comparisons of artworks, datasets, and classification schemes</td>
        <td style="padding: 0.8rem; color: #000;">Demonstrates gap between human meaning and AI classification</td>
        <td style="padding: 0.8rem;">Conceptual art, Semiotic analysis, Visual rhetoric</td>
      </tr>
      <tr style="border-bottom: 1px solid #dee2e6; background: #ffffff;">
        <td style="padding: 0.8rem; font-weight: 600; color: #2c3e50;">Scale Demonstration</td>
        <td style="padding: 0.8rem;">Shows massive scope of dataset collection through visual density and repetition</td>
        <td style="padding: 0.8rem; color: #000;">Reveals overwhelming scale of data extraction and classification</td>
        <td style="padding: 0.8rem;">Big data visualization, Information overload art</td>
      </tr>
      <tr style="border-bottom: 1px solid #dee2e6;">
        <td style="padding: 0.8rem; font-weight: 600; color: #2c3e50;">Forensic Aesthetic</td>
        <td style="padding: 0.8rem;">Clinical, systematic presentation that mimics investigative documentation and evidence gathering</td>
        <td style="padding: 0.8rem; color: #000;">Establishes credibility and authority for critical analysis</td>
        <td style="padding: 0.8rem;">Forensic architecture, Evidence-based art, Critical documentation</td>
      </tr>
      <tr style="background: #ffffff;">
        <td style="padding: 0.8rem; font-weight: 600; color: #2c3e50;">Systematic Exposure</td>
        <td style="padding: 0.8rem;">Makes hidden classification processes visible through systematic visual analysis</td>
        <td style="padding: 0.8rem; color: #000;">Reveals structural violence embedded in AI systems</td>
        <td style="padding: 0.8rem;">Algorithmic Justice League, Data & Society, Critical data studies</td>
      </tr>
  </tbody>
</table>

<div style="background: #fff; padding: 2rem; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); margin: 1.5rem 0; border-left: 5px solid #2c3e50;">
  <p style="margin: 0; font-size: 1.1rem; line-height: 1.7; color: #000;">
    <strong>Strategic Visual Philosophy:</strong> Excavating AI's aesthetic choices are fundamentally strategic rather than merely stylistic—every visual element serves the critical analysis. The project situates itself within a vibrant interdisciplinary ecosystem that combines art, design, activism, and research to interrogate AI power dynamics. The minimalist approach ensures that raw dataset content remains the primary focus, while systematic presentation makes complex technical issues accessible to diverse audiences. By using visual clarity and scale to provoke reflection on the ethical foundations of machine learning, the project demonstrates how aesthetic decisions can be powerful tools for social critique and technological accountability.
  </p>
</div>
</section>

<!-- Diagram Analysis -->
<section id="diagramAnalysis">
  <h2>Relational Structure Analysis</h2>
  <p>
    This UML diagram visualizes the complex network of relationships and power dynamics within Excavating AI's dataset ecosystem. It maps the key entities involved in AI training data production, from the datasets themselves to the human subjects whose images are used, and shows how these elements interact to create and perpetuate bias in machine learning systems. The diagram reveals the hidden labor behind dataset creation, the exploitative relationships between tech companies and workers, and the ways in which AI models inherit and amplify social inequalities through their training data.
  </p>
  <p>
    Each class in the diagram represents a critical component of the AI training pipeline: the massive datasets that form the foundation of machine learning, the invisible Mechanical Turk workers who label the data, the AI models that learn from biased training sets, the human subjects whose images become data without their knowledge, and the critical intervention of Excavating AI itself. The relationships between these entities expose the structural problems in AI development, from the lack of consent in data collection to the reproduction of historical discrimination through algorithmic systems.
  </p>
  <div id="uml-diagram" style="width: 100%; max-width: 100%; margin: 0.5em 0; background: #ffffff; border-radius: 8px; padding: 5px; text-align: center;"></div>

  <h2>Methodology & Technical Approach</h2>

  <p style="margin: 1.5rem 0; line-height: 1.6; color: #000;">
    The methodology underlying Excavating AI represents a sophisticated hybrid approach combining computational analysis, critical theory, and visual investigation. Crawford and Paglen employed a multi-layered investigative framework that systematically deconstructed AI training datasets to reveal their embedded biases and cultural assumptions. Their approach was fundamentally interdisciplinary, drawing from computer science, sociology, art history, and critical theory to create a comprehensive critique of machine learning systems.
  </p>

  <table style="width: 100%; border-collapse: collapse; margin: 1.5rem 0; background: #f8f9fa; border-radius: 8px; overflow: hidden; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
    <thead>
      <tr style="background: linear-gradient(90deg, #495057, #6c757d); color: white;">
        <th style="padding: 0.8rem; text-align: left; font-weight: 600; width: 20%;">Component</th>
        <th style="padding: 0.8rem; text-align: left; font-weight: 600; width: 50%;">Description</th>
        <th style="padding: 0.8rem; text-align: left; font-weight: 600; width: 30%;">Impact</th>
      </tr>
    </thead>
    <tbody>
      <tr style="border-bottom: 1px solid #dee2e6;">
        <td style="padding: 0.8rem; font-weight: 600; color: #495057;">Tech Stack</td>
        <td style="padding: 0.8rem;">D3.js for interactive visualizations, Python for data mining and analysis, web scraping tools for dataset collection, custom HTML/CSS for responsive design, statistical analysis packages for bias quantification</td>
        <td style="padding: 0.8rem; color: #000;">Created reproducible, transparent investigation methods that could be replicated by other researchers</td>
      </tr>
      <tr style="border-bottom: 1px solid #dee2e6; background: #ffffff;">
        <td style="padding: 0.8rem; font-weight: 600; color: #495057;">Data Sources</td>
        <td style="padding: 0.8rem;">ImageNet (14M images, 22,000 categories), MegaFace (4M faces, 672,057 identities), UTKFace (20,000+ images with age/gender/race labels), MS-Celeb-1M (10M celebrity faces), WordNet taxonomy, Mechanical Turk labeling data</td>
        <td style="padding: 0.8rem; color: #000;">Exposed scale and scope of problematic datasets, revealed hidden labor practices, documented consent violations</td>
      </tr>
      <tr style="border-bottom: 1px solid #dee2e6;">
        <td style="padding: 0.8rem; font-weight: 600; color: #495057;">Theoretical Framework</td>
        <td style="padding: 0.8rem;">Foucault's biopower and classification systems, feminist standpoint theory, critical race theory, semiotic analysis, visual rhetoric, postcolonial theory</td>
        <td style="padding: 0.8rem; color: #000;">Provided intellectual foundation for critique, connected technical analysis to broader social theory</td>
      </tr>
      <tr style="border-bottom: 1px solid #dee2e6; background: #ffffff;">
        <td style="padding: 0.8rem; font-weight: 600; color: #495057;">Visual Methodology</td>
        <td style="padding: 0.8rem;">Combined quantitative dataset analysis with qualitative visual critique, systematic image sampling, taxonomic visualization, comparative analysis across datasets</td>
        <td style="padding: 0.8rem; color: #000;">Made complex technical issues accessible to broader audiences, created compelling visual evidence</td>
      </tr>
      <tr style="background: #ffffff;">
        <td style="padding: 0.8rem; font-weight: 600; color: #495057;">Research Process</td>
        <td style="padding: 0.8rem;">Systematic dataset collection, bias quantification, labor practice investigation, consent analysis, comparative taxonomy study, visual documentation</td>
        <td style="padding: 0.8rem; color: #000;">Established new research methodology for AI ethics investigation, created template for future studies</td>
      </tr>
    </tbody>
  </table>

  <p style="margin: 1.5rem 0; line-height: 1.6; color: #000;">
    The project's technical implementation involved sophisticated data mining techniques to extract and analyze large-scale datasets. They developed custom tools to scrape and process millions of images, systematically documented labeling practices, and created interactive visualizations that made their findings accessible to both technical and non-technical audiences. The methodology emphasized transparency and reproducibility, with all tools and datasets made publicly available for verification and further study.
  </p>

  <h2>Rhetorical Analysis</h2>

  <p style="margin: 1.5rem 0; line-height: 1.6; color: #000;">
    Excavating AI employs a sophisticated rhetorical strategy that combines empirical evidence with critical theory to challenge fundamental assumptions about AI systems. The project's argumentative structure is built around systematic exposure of bias, labor exploitation, and epistemological problems in machine learning datasets. Through careful selection and presentation of evidence, Crawford and Paglen construct a compelling case that AI systems are not neutral technological tools but cultural artifacts that inherit and amplify existing social inequalities.
  </p>

  <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem; margin: 1.5rem 0;">
    <div style="background: #fff; padding: 1.5rem; border-radius: 8px; border-left: 5px solid #dc3545; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
      <h4 style="margin: 0 0 1rem 0; color: #dc3545; font-size: 1.1rem;">Core Argument</h4>
      <p style="margin: 0; line-height: 1.6;">AI systems are not neutral technological tools but cultural artifacts that inherit and amplify existing social biases through their training datasets. The project argues that the foundational datasets used to train machine learning systems encode harmful stereotypes, outdated taxonomies, and discriminatory practices that perpetuate inequality at scale.</p>
    </div>
    <div style="background: #fff; padding: 1.5rem; border-radius: 8px; border-left: 5px solid #1e3a8a; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
      <h4 style="margin: 0 0 1rem 0; color: #1e3a8a; font-size: 1.1rem;">Critical Approach</h4>
      <p style="margin: 0; line-height: 1.6;">Multi-dimensional critique of AI's technical foundations, political economy, epistemological assumptions, and social impact. The project challenges dominant narratives about AI's objectivity and neutrality through systematic analysis and visual evidence.</p>
    </div>
  </div>

  <table style="width: 100%; border-collapse: collapse; margin: 1.5rem 0; background: #fff; border-radius: 8px; overflow: hidden; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
    <thead>
      <tr style="background: linear-gradient(90deg, #4A148C, #6A1B9A); color: white;">
        <th style="padding: 0.8rem; text-align: left; font-weight: 600; width: 10%;">#</th>
        <th style="padding: 0.8rem; text-align: left; font-weight: 600; width: 90%;">Key Claims & Evidence</th>
      </tr>
    </thead>
    <tbody>
      <tr style="border-bottom: 1px solid #dee2e6;">
        <td style="padding: 0.8rem; font-weight: 600; color: #4A148C; text-align: center;">1</td>
        <td style="padding: 0.8rem;">Dataset construction is a form of social classification that reflects cultural power dynamics. Evidence: ImageNet's "Person" categories include harmful labels like "Hermaphrodite," "Kleptomaniac," and "Anti-Semite," demonstrating how classification systems create and perpetuate stereotypes.</td>
      </tr>
      <tr style="border-bottom: 1px solid #dee2e6; background: #f8f9fa;">
        <td style="padding: 0.8rem; font-weight: 600; color: #4A148C; text-align: center;">2</td>
        <td style="padding: 0.8rem;">The labor behind dataset creation (Mechanical Turk workers) is often invisible and exploitative. Evidence: Documentation of underpaid labeling work, lack of worker protections, and the hidden human cost of AI training data production.</td>
      </tr>
      <tr style="border-bottom: 1px solid #dee2e6;">
        <td style="padding: 0.8rem; font-weight: 600; color: #4A148C; text-align: center;">3</td>
        <td style="padding: 0.8rem;">AI systems reproduce historical forms of discrimination through their training data. Evidence: UTKFace dataset's binary gender and racial classifications echo historical pseudoscience, while IBM's "Diversity in Faces" still uses problematic measurement systems.</td>
      </tr>
      <tr style="border-bottom: 1px solid #dee2e6; background: #f8f9fa;">
        <td style="padding: 0.8rem; font-weight: 600; color: #4A148C; text-align: center;">4</td>
        <td style="padding: 0.8rem;">The gap between image and label reveals fundamental epistemological problems in computer vision. Evidence: Comparison of Agnes Martin's "White Flower" and Magritte's "The Treachery of Images" demonstrates the inadequacy of labels to capture image meaning.</td>
      </tr>
      <tr style="background: #f8f9fa;">
        <td style="padding: 0.8rem; font-weight: 600; color: #4A148C; text-align: center;">5</td>
        <td style="padding: 0.8rem;">Accountability and transparency are essential for ethical AI development. Evidence: The removal of MS-Celeb-1M dataset after privacy violations demonstrates the need for better oversight and consent practices in dataset construction.</td>
      </tr>
    </tbody>
  </table>

  <p style="margin: 1.5rem 0; line-height: 1.6; color: #000;">
    The project's rhetorical effectiveness stems from its combination of quantitative evidence (dataset statistics, bias measurements) with qualitative analysis (visual critique, historical context). By presenting concrete examples of problematic classifications alongside theoretical frameworks, Excavating AI makes abstract concepts tangible and compelling. The visual presentation of evidence—showing actual images and labels from datasets—creates immediate emotional and intellectual impact that statistics alone could not achieve.
  </p>

  <h2>Authors' Broader Practice</h2>

  <p style="margin: 1.5rem 0; line-height: 1.6; color: #000;">
    Excavating AI represents a pivotal moment in both authors' careers, marking a convergence of their distinct research trajectories and establishing new directions for their subsequent work. The collaboration between Crawford's academic research background and Paglen's artistic practice created a unique interdisciplinary approach that has influenced both fields. Understanding how this project fits within their broader oeuvre reveals the evolution of their thinking and the impact of their collaboration on critical AI studies.
  </p>

  <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem; margin: 1.5rem 0;">
    <div style="background: #fff; padding: 1.5rem; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); border-top: 4px solid #2E8B57;">
      <h4 style="margin: 0 0 1rem 0; color: #2E8B57; font-size: 1.2rem;">Kate Crawford's Evolution</h4>
      <ul style="margin: 0; padding-left: 1.2rem; line-height: 1.6;">
        <li><strong>Early Work:</strong> Research on social media, digital labor, and platform economies (2010-2018)</li>
        <li><strong>Excavating AI:</strong> First major foray into AI ethics and dataset analysis (2019)</li>
        <li><strong>Atlas of AI:</strong> Expanded investigation into AI's material infrastructure (2021)</li>
        <li><strong>Current Focus:</strong> AI Now Institute leadership, policy advocacy, environmental AI impacts</li>
        <li><strong>Methodological Development:</strong> From qualitative social research to computational critical analysis</li>
      </ul>
    </div>
    <div style="background: #fff; padding: 1.5rem; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); border-top: 4px solid #2E8B57;">
      <h4 style="margin: 0 0 1rem 0; color: #2E8B57; font-size: 1.2rem;">Trevor Paglen's Journey</h4>
      <ul style="margin: 0; padding-left: 1.2rem; line-height: 1.6;">
        <li><strong>Early Work:</strong> Photography of classified military sites and surveillance infrastructure (2005-2015)</li>
        <li><strong>Surveillance Studies:</strong> Documentation of mass surveillance systems and data centers</li>
        <li><strong>Excavating AI:</strong> Transition from physical to digital surveillance critique</li>
        <li><strong>Current Focus:</strong> AI classification systems, algorithmic bias, digital surveillance art</li>
        <li><strong>Artistic Evolution:</strong> From documentary photography to computational art and data visualization</li>
      </ul>
    </div>
  </div>

  <table style="width: 100%; border-collapse: collapse; margin: 1.5rem 0; background: #f8f9fa; border-radius: 8px; overflow: hidden; box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
    <thead>
      <tr style="background: linear-gradient(90deg, #2E8B57, #228B22); color: white;">
        <th style="padding: 0.8rem; text-align: left; font-weight: 600; width: 25%;">Collaborative Impact</th>
        <th style="padding: 0.8rem; text-align: left; font-weight: 600; width: 75%;">Influence on Subsequent Work</th>
      </tr>
    </thead>
    <tbody>
      <tr style="border-bottom: 1px solid #dee2e6;">
        <td style="padding: 0.8rem; font-weight: 600; color: #495057;">Methodological Innovation</td>
        <td style="padding: 0.8rem;">Established interdisciplinary approach combining computational analysis with critical theory, influencing both academic research and artistic practice in AI ethics</td>
      </tr>
      <tr style="border-bottom: 1px solid #dee2e6; background: #ffffff;">
        <td style="padding: 0.8rem; font-weight: 600; color: #495057;">Visual Communication</td>
        <td style="padding: 0.8rem;">Demonstrated how complex technical issues could be communicated through compelling visual narratives, setting new standards for research presentation</td>
      </tr>
      <tr style="border-bottom: 1px solid #dee2e6;">
        <td style="padding: 0.8rem; font-weight: 600; color: #495057;">Academic-Artistic Bridge</td>
        <td style="padding: 0.8rem;">Created template for collaboration between researchers and artists, influencing subsequent projects in critical data studies and AI ethics</td>
      </tr>
      <tr style="background: #ffffff;">
        <td style="padding: 0.8rem; font-weight: 600; color: #495057;">Policy Influence</td>
        <td style="padding: 0.8rem;">Directly influenced AI ethics guidelines, dataset transparency requirements, and responsible AI development practices in both industry and academia</td>
      </tr>
    </tbody>
  </table>

  <h2>Personal Assessment</h2>

  <p style="margin: 1.5rem 0; line-height: 1.6; color: #000;">
    Excavating AI represents a landmark contribution to critical AI studies that has fundamentally shaped the field's development. The project's impact extends beyond academic research to influence policy, industry practices, and public understanding of AI systems. However, like any significant work, it has both notable strengths and important limitations that warrant careful consideration.
  </p>

  <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem; margin: 1.5rem 0;">
    <div style="background: #fff; padding: 1.5rem; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); border-top: 4px solid #28a745;">
      <h4 style="margin: 0 0 1rem 0; color: #28a745; font-size: 1.2rem;">Methodological Strengths</h4>
      <ul style="margin: 0; padding-left: 1.2rem; line-height: 1.6;">
        <li><strong>Innovative Approach:</strong> Created new methodology for investigating AI systems that combines computational analysis with critical theory</li>
        <li><strong>Perfect Timing:</strong> Emerged during critical period when AI ethics was gaining mainstream attention</li>
        <li><strong>Accessibility:</strong> Made complex technical issues accessible to broader audiences through visual storytelling</li>
        <li><strong>Root Cause Focus:</strong> Addressed fundamental dataset issues rather than just algorithmic symptoms</li>
        <li><strong>Reproducibility:</strong> Provided transparent methodology that others could replicate and build upon</li>
      </ul>
    </div>
    <div style="background: #fff; padding: 1.5rem; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); border-top: 4px solid #dc3545;">
      <h4 style="margin: 0 0 1rem 0; color: #dc3545; font-size: 1.2rem;">Analytical Limitations</h4>
      <ul style="margin: 0; padding-left: 1.2rem; line-height: 1.6;">
        <li><strong>Limited Intersectionality:</strong> Could have better examined how different forms of bias interact and compound</li>
        <li><strong>Dataset Scope:</strong> Focus on large datasets may miss smaller specialized datasets that also perpetuate bias</li>
        <li><strong>Solution Gap:</strong> More critique than constructive solutions for creating better datasets</li>
        <li><strong>Access Limitations:</strong> Only examined publicly available datasets, missing proprietary tech company data</li>
        <li><strong>Geographic Focus:</strong> Primarily focused on Western datasets and classification systems</li>
      </ul>
    </div>
  </div>

  <h2>Overall Impact & Legacy</h2>

  <div style="background: #fff; padding: 2rem; border-radius: 12px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); margin: 1.5rem 0; border-left: 5px solid #9b59b6;">
    <p style="margin: 0; font-size: 1.1rem; line-height: 1.7; color: #000;">
      <strong>Fundamental Paradigm Shift:</strong> Excavating AI fundamentally changed how we understand and discuss AI bias. The project's legacy is evident across multiple domains: it has influenced academic research methodologies, shaped policy development around AI ethics, raised public awareness about dataset bias, and established new standards for responsible AI development. The work's fingerprints can be seen in subsequent research on algorithmic bias, dataset transparency initiatives, and critical data studies. Perhaps most significantly, it demonstrated that rigorous technical analysis and compelling visual communication could work together to create meaningful social impact. Excavating AI stands as one of those rare projects that not only identified critical problems but also created the tools and frameworks for others to continue the investigation, ensuring its influence will continue to grow as AI systems become more pervasive in society.
    </p>
  </div>
</section>

<script>
// Show/hide tabs
function showSection(id) {
  document.querySelectorAll('section').forEach(sec => sec.classList.remove('active'));
  document.getElementById(id).classList.add('active');
  document.querySelectorAll('nav button').forEach(btn => btn.classList.remove('active'));
  if (id === 'dataCollection') document.getElementById('tab1').classList.add('active');
  if (id === 'layersAnalysis') document.getElementById('tab2').classList.add('active');
  if (id === 'diagramAnalysis') document.getElementById('tab3').classList.add('active');
}

// Timeline data
const data = [
  {
    year: 2018,
    event: "AI Now Report 2018 questions dataset bias",
    details: "The AI Now Report identified how datasets embed social biases. It emphasized transparency in dataset construction. The report noted facial recognition can reinforce discrimination. It became a foundational text in AI ethics debates."
  },
  {
    year: 2018,
    event: "Gender Shades study highlights facial recognition bias",
    details: "Buolamwini and Gebru showed error rates up to 34% for dark-skinned women. Training data lacked diversity. They called for auditing AI performance across demographics. This spurred companies to reevaluate facial analysis systems."
  },
  {
    year: 2019,
    event: "Excavating AI published by Crawford & Paglen",
    details: "Excavating AI exposed how ImageNet relied on biased taxonomies. It argued datasets are socially constructed. The work connected AI to histories of classification. It blended essays with visual critique."
  },
  {
    year: 2019,
    event: "Microsoft deletes MS-Celeb-1M dataset",
    details: "Microsoft removed the MS-Celeb dataset of 10 million faces. Images were scraped without consent. Deletion highlighted privacy concerns. The event intensified scrutiny of data sourcing practices."
  },
  {
    year: 2020,
    event: "Public debates on facial recognition ethics intensify",
    details: "Reports showed concern over face datasets in academia and corporations. Activists called for moratoriums on facial recognition. Media coverage broadened the debate. This marked a shift to mainstream awareness."
  },
  {
    year: 2021,
    event: "Atlas of AI book published by Kate Crawford",
    details: "Atlas of AI examined extraction in AI—from minerals to human labor. Crawford detailed the political economy of data. The book argued AI perpetuates inequality by design. It became an influential critique."
  },
  {
    year: 2023,
    event: "Calls for dataset transparency increase",
    details: "Researchers published analyses of large image datasets. They highlighted undocumented practices and bias. Papers on arXiv called for dataset audits. Transparency became a central demand in AI ethics."
  },
  {
    year: 2025,
    event: "Ongoing debates on AI fairness and surveillance",
    details: "Debates continue over algorithmic fairness and consent. Policymakers consider legislation for ethical data practices. Research scrutinizes long-term impacts of facial recognition. The field grapples with accountability."
  }
];

function drawTimeline() {
  const margin = { top: 20, right: 20, bottom: 20, left: 120 };
  const width = 900 - margin.left - margin.right;
  const height = data.length * 100;

  const svg = d3.select("#timeline")
    .append("svg")
    .attr("width", width + margin.left + margin.right)
    .attr("height", height + margin.top + margin.bottom);

  const g = svg.append("g")
    .attr("transform", `translate(${margin.left},${margin.top})`);

  const y = d3.scaleLinear()
    .domain([0, data.length - 1])
    .range([0, height - margin.top - margin.bottom]);

  // Timeline vertical line
  g.append("line")
    .attr("x1", 0)
    .attr("y1", 0)
    .attr("x2", 0)
    .attr("y2", height - margin.top - margin.bottom)
    .attr("stroke", "#0645ad")
    .attr("stroke-width", 2);

  const events = g.selectAll(".timeline-group")
    .data(data)
    .enter()
    .append("g")
    .attr("class", "timeline-group")
    .attr("transform", (d, i) => `translate(0, ${y(i)})`);

  // Circles
  events.append("circle")
    .attr("cx", 0)
    .attr("cy", 0)
    .attr("r", 8)
    .attr("fill", "#0645ad")
    .on("mouseover", showTooltip)
    .on("mouseout", hideTooltip);

  // Year labels
  events.append("text")
    .attr("class", "year")
    .attr("x", -10)
    .attr("y", 5)
    .attr("text-anchor", "end")
    .text(d => d.year)
    .on("mouseover", showTooltip)
    .on("mouseout", hideTooltip);

  // Event text with wrapping
  events.append("text")
    .attr("class", "event")
    .attr("x", 20)
    .attr("y", 0)
    .attr("dy", "0.35em")
    .style("font-size", "1em")
    .style("fill", "#333")
    .text(d => d.event)
    .on("mouseover", showTooltip)
    .on("mouseout", hideTooltip);

  // Tooltip div
  const tooltip = d3.select("body").append("div")
    .attr("id", "tooltip")
    .style("opacity", 0)
    .style("position", "absolute")
    .style("text-align", "left")
    .style("max-width", "300px")
    .style("padding", "8px")
    .style("font-size", "0.9em")
    .style("background", "#fff")
    .style("border", "1px solid #ccc")
    .style("border-radius", "4px")
    .style("pointer-events", "none")
    .style("box-shadow", "0 0 5px rgba(0,0,0,0.3)")
    .style("line-height", "1.4");

  function showTooltip(event, d) {
    tooltip.transition().duration(200).style("opacity", 0.95);
    tooltip.html(
      `<strong>${d.year}</strong><br/>
       <em>${d.event}</em><br/><br/>${d.details}`
    )
      .style("left", (event.pageX + 15) + "px")
      .style("top", (event.pageY - 28) + "px");
  }

  function hideTooltip() {
    tooltip.transition().duration(300).style("opacity", 0);
  }
}
drawTimeline();

// UML Class Diagram for Excavating AI Dataset Ecosystem
function drawUMLDiagram() {
  const width = Math.min(window.innerWidth - 40, 1200);
  const height = 750;
  
  const svg = d3.select("#uml-diagram")
    .append("svg")
    .attr("width", width)
    .attr("height", height);

  // Define the classes for the UML diagram with comprehensive content
  const classes = [
    {
      id: "Datasets",
      x: (width - 220) / 2,
      y: 60,
      width: 220,
      height: 160,
      name: "Datasets",
      attributes: ["ImageNet: 14M images", "MegaFace: 4M faces", "UTKFace: Age/Gender/Race", "MS-Celeb: 10M faces"],
      methods: ["train()", "classify()", "bias()", "stereotype()"],
      color: "#3498db"
    },
    {
      id: "MechanicalTurk",
      x: (width - 800) / 2,
      y: 280,
      width: 220,
      height: 160,
      name: "Mechanical Turk",
      attributes: ["workers: Array<Human>", "tasks: Labeling", "compensation: Low", "visibility: Hidden"],
      methods: ["assign()", "pay()", "exploit()", "label()"],
      color: "#e67e22"
    },
    {
      id: "AI_Model",
      x: (width - 220) / 2,
      y: 280,
      width: 220,
      height: 160,
      name: "AI Model",
      attributes: ["algorithm: String", "training_data: Array<Dataset>", "bias: Inherited", "epistemics: Cultural"],
      methods: ["learn()", "predict()", "discriminate()", "misread()"],
      color: "#9b59b6"
    },
    {
      id: "HumanSubject",
      x: (width + 360) / 2,
      y: 280,
      width: 220,
      height: 160,
      name: "Human Subject",
      attributes: ["identity: Object", "consent: Boolean", "status: Data", "agency: Limited"],
      methods: ["resist()", "claim()", "exist()", "name()"],
      color: "#27ae60"
    },
    {
      id: "ExcavatingAI",
      x: (width - 220) / 2,
      y: 500,
      width: 220,
      height: 160,
      name: "Excavating AI",
      attributes: ["evidence: Array<Dataset>", "arguments: Array<String>", "method: Visual", "impact: Accountability"],
      methods: ["expose()", "challenge()", "transform()", "advocate()"],
      color: "#e74c3c"
    }
  ];

  // Define relationships between classes with comprehensive analysis
  const relationships = [
    { source: "Datasets", target: "AI_Model", type: "1", multiplicity: "1..*" },
    
    { source: "MechanicalTurk", target: "Datasets", type: "2", multiplicity: "1" },
    
    { source: "HumanSubject", target: "Datasets", type: "3", multiplicity: "0..*" },
    
    { source: "ExcavatingAI", target: "HumanSubject", type: "5", multiplicity: "0..*" },
    { source: "ExcavatingAI", target: "AI_Model", type: "6", multiplicity: "1..*" }
  ];

  // Add arrow marker for relationships
  svg.append("defs").append("marker")
    .attr("id", "arrowhead")
    .attr("viewBox", "0 -5 10 10")
    .attr("refX", 8)
    .attr("refY", 0)
    .attr("orient", "auto")
    .attr("markerWidth", 6)
    .attr("markerHeight", 6)
    .append("path")
    .attr("d", "M0,-5L10,0L0,5")
    .attr("fill", "#2c3e50")
    .attr("stroke", "#2c3e50")
    .attr("stroke-width", "1");

  // Function to get relationship descriptions
  function getRelationshipDescription(type) {
    const descriptions = {
      "1": "trains",
      "2": "labels", 
      "3": "becomes data",
      "5": "advocates for",
      "6": "critiques"
    };
    return descriptions[type] || "unknown";
  }

  // Draw relationships (lines) with clean edge connections
  relationships.forEach(rel => {
    const sourceClass = classes.find(c => c.id === rel.source);
    const targetClass = classes.find(c => c.id === rel.target);
    
    if (sourceClass && targetClass) {
      // Calculate clean connection points at edges
      let x1, y1, x2, y2;
      
      // Simple, clean connection points to box edges
      if (sourceClass.y < targetClass.y - 50) {
        // Source above target: connect bottom of source to top of target
        x1 = sourceClass.x + sourceClass.width/2;
        y1 = sourceClass.y + sourceClass.height;
        x2 = targetClass.x + targetClass.width/2;
        y2 = targetClass.y;
      } else if (sourceClass.y > targetClass.y + 50) {
        // Source below target: connect top of source to bottom of target
        x1 = sourceClass.x + sourceClass.width/2;
        y1 = sourceClass.y;
        x2 = targetClass.x + targetClass.width/2;
        y2 = targetClass.y + targetClass.height;
      } else {
        // Same row: connect side edges
        if (sourceClass.x < targetClass.x) {
          x1 = sourceClass.x + sourceClass.width;
          y1 = sourceClass.y + sourceClass.height/2;
          x2 = targetClass.x;
          y2 = targetClass.y + targetClass.height/2;
        } else {
          x1 = sourceClass.x;
          y1 = sourceClass.y + sourceClass.height/2;
          x2 = targetClass.x + targetClass.width;
          y2 = targetClass.y + targetClass.height/2;
        }
      }
      
      // Draw relationship line with solid styling for better visibility
      const line = svg.append("line")
        .attr("x1", x1)
        .attr("y1", y1)
        .attr("x2", x2)
        .attr("y2", y2)
        .attr("stroke", "#2c3e50")
        .attr("stroke-width", 3)
        .attr("marker-end", "url(#arrowhead)");

      // Add relationship label with better positioning and clarity
      const midX = (x1 + x2) / 2;
      const midY = (y1 + y2) / 2;

      // Add relationship label with smart positioning to avoid box overlap
      let labelY = midY - 15;
      
      // Special positioning for "advocates for" relationship
      if (getRelationshipDescription(rel.type) === "advocates for") {
        // Position between AI Model and Human Subject
        labelY = midY - 30;
      }
      
      // Check if label would overlap with any box
      const labelHeight = 20;
      const labelWidth = getRelationshipDescription(rel.type).length * 7; // Approximate width
      
      classes.forEach(cls => {
        if (midX >= cls.x - labelWidth/2 && midX <= cls.x + cls.width + labelWidth/2 &&
            labelY >= cls.y - labelHeight && labelY <= cls.y + cls.height + labelHeight) {
          // Label would overlap, move it further away
          if (midY < cls.y) {
            labelY = cls.y - 30; // Move above box
          } else {
            labelY = cls.y + cls.height + 30; // Move below box
          }
        }
      });
      
      // Default styling for relationship labels
      let labelColor = "#2c3e50";
      let labelSize = "16px";
      
      // Use center position for all labels
      let labelX = midX;
      
      svg.append("text")
        .attr("x", labelX)
        .attr("y", labelY + 2)
        .attr("text-anchor", "middle")
        .attr("font-size", labelSize)
        .attr("font-weight", "bold")
        .attr("fill", labelColor)
        .attr("stroke", "#ffffff")
        .attr("stroke-width", "3px")
        .attr("paint-order", "stroke")
        .text(getRelationshipDescription(rel.type));
    }
  });

  // Draw class boxes
  classes.forEach(cls => {
    const group = svg.append("g");

    // Class box with color
    group.append("rect")
      .attr("x", cls.x)
      .attr("y", cls.y)
      .attr("width", cls.width)
      .attr("height", cls.height)
      .attr("fill", cls.color)
      .attr("stroke", "#333")
      .attr("stroke-width", 2)
      .attr("opacity", 0.9);

    // Class name
    group.append("text")
      .attr("x", cls.x + cls.width/2)
      .attr("y", cls.y + 18)
      .attr("text-anchor", "middle")
      .attr("font-weight", "bold")
      .attr("font-size", "16px")
      .attr("fill", "#000")
      .text(cls.name);

    // Separator line
    group.append("line")
      .attr("x1", cls.x)
      .attr("y1", cls.y + 22)
      .attr("x2", cls.x + cls.width)
      .attr("y2", cls.y + 22)
      .attr("stroke", "#000")
      .attr("stroke-width", 2);

    // Attributes
    cls.attributes.forEach((attr, i) => {
      group.append("text")
        .attr("x", cls.x + 5)
        .attr("y", cls.y + 38 + (i * 13))
        .attr("font-size", cls.id === "AI_Model" ? "13px" : "12px")
        .attr("fill", "#000")
        .attr("font-weight", "normal")
        .text(attr);
    });

    // Methods separator
    const methodStartY = cls.y + 22 + (cls.attributes.length * 13) + 12;
    group.append("line")
      .attr("x1", cls.x)
      .attr("y1", methodStartY)
      .attr("x2", cls.x + cls.width)
      .attr("y2", methodStartY)
      .attr("stroke", "#000")
      .attr("stroke-width", 2);

    // Methods
    cls.methods.forEach((method, i) => {
      group.append("text")
        .attr("x", cls.x + 5)
        .attr("y", methodStartY + 15 + (i * 13))
        .attr("font-size", cls.id === "AI_Model" ? "13px" : "12px")
        .attr("fill", "#000")
        .attr("font-weight", "normal")
        .text(method + "()");
    });
  });

  // Add title
  svg.append("text")
    .attr("x", width/2)
    .attr("y", 25)
    .attr("text-anchor", "middle")
    .attr("font-size", "16px")
    .attr("font-weight", "bold")
    .text("Excavating AI: UML Class Diagram");
}

// Initialize UML diagram when page loads
drawUMLDiagram();
</script>
</body>
</html>